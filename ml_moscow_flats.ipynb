{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подключаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic \n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, median_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объявляем функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляет азимут по двум заданным точкам, код взят отсюда: https://pastebin.com/PHeWmiEN\n",
    "def get_azimuth(latitude, longitude):\n",
    " \n",
    "    rad = 6372795\n",
    "\n",
    "    llat1 = city_center_coordinates[0]\n",
    "    llong1 = city_center_coordinates[1]\n",
    "    llat2 = latitude\n",
    "    llong2 = longitude\n",
    "\n",
    "    lat1 = llat1*math.pi/180.\n",
    "    lat2 = llat2*math.pi/180.\n",
    "    long1 = llong1*math.pi/180.\n",
    "    long2 = llong2*math.pi/180.\n",
    "\n",
    "    cl1 = math.cos(lat1)\n",
    "    cl2 = math.cos(lat2)\n",
    "    sl1 = math.sin(lat1)\n",
    "    sl2 = math.sin(lat2)\n",
    "    delta = long2 - long1\n",
    "    cdelta = math.cos(delta)\n",
    "    sdelta = math.sin(delta)\n",
    "\n",
    "    y = math.sqrt(math.pow(cl2*sdelta,2)+math.pow(cl1*sl2-sl1*cl2*cdelta,2))\n",
    "    x = sl1*sl2+cl1*cl2*cdelta\n",
    "    ad = math.atan2(y,x)\n",
    "\n",
    "    x = (cl1*sl2) - (sl1*cl2*cdelta)\n",
    "    y = sdelta*cl2\n",
    "    z = math.degrees(math.atan(-y/x))\n",
    "\n",
    "    if (x < 0):\n",
    "        z = z+180.\n",
    "\n",
    "    z2 = (z+180.) % 360. - 180.\n",
    "    z2 = - math.radians(z2)\n",
    "    anglerad2 = z2 - ((2*math.pi)*math.floor((z2/(2*math.pi))) )\n",
    "    angledeg = (anglerad2*180.)/math.pi\n",
    "    \n",
    "    return round(angledeg, 2)\n",
    "\n",
    "#Вычисляет среднюю абсолютную процентную ошибку\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#Вычисляет медианную абсолютную процентную ошибку\n",
    "def median_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.median(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#Печатает рассчитанные значения коэффициента детерминации и ошибок\n",
    "def print_metrics(prediction, val_y):\n",
    "    val_mae = mean_absolute_error(val_y, prediction)\n",
    "    median_AE = median_absolute_error(val_y, prediction)\n",
    "    r2 = r2_score(val_y, prediction)\n",
    "\n",
    "    print('')\n",
    "    print('R\\u00b2: {:.2}'.format(r2))\n",
    "    print('')\n",
    "    print('Средняя абсолютная ошибка: {:.3} %'.format(mean_absolute_percentage_error(val_y, prediction)))\n",
    "    print('Медианная абсолютная ошибка: {:.3} %'.format(median_absolute_percentage_error(val_y, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем датасет и делаем первичную обработку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#При помощи библиотеки pandas считываем csv-файл и преобразуем его в формат датафрейма (таблицы)\n",
    "file_path = 'https://raw.githubusercontent.com/maxbobkov/ml_moscow_flats/master/moscow_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Выводим 5 первых строк датафрейма\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем новый столбец Стоимость 1 кв.м путем построчного деления стоимостей квартир на их общие площади\n",
    "df['priceMetr'] = df['price']/df['totalArea']\n",
    "\n",
    "#Задаем широту и долготу центра города и рассчитываем для каждой квартиры расстояние от центра и азимут \n",
    "city_center_coordinates = [55.7522, 37.6156]\n",
    "df['distance'] = list(map(lambda x, y: geodesic(city_center_coordinates, [x, y]).meters, df['latitude'], df['longitude']))\n",
    "df['azimuth'] = list(map(lambda x, y: get_azimuth(x, y), df['latitude'], df['longitude']))\n",
    "\n",
    "#Выбираем из датафрейма только те квартиры, которые расположены не дальше 40 км от центра города и объявления о продаже которых были созданы не ранее 1 июня 2019 г. \n",
    "df = df.loc[(df['distance'] < 40000) & (df['creationDate'] > '2019-06-01')]\n",
    "\n",
    "#Выводим сводную информацию о датафрейме и его столбцах (признаках)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Удаляем выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляем строки со значениями-выбросами \n",
    "first_quartile = df.quantile(q=0.25)\n",
    "third_quartile = df.quantile(q=0.75)\n",
    "IQR = third_quartile - first_quartile\n",
    "outliers = df[(df > (third_quartile + 1.5 * IQR)) | (df < (first_quartile - 1.5 * IQR))].count(axis=1)\n",
    "outliers.sort_values(axis=0, ascending=False, inplace=True)\n",
    "\n",
    "#Удаляем из датафрейма 500 строк, подходящих под критерии выбросов\n",
    "outliers = outliers.head(500)\n",
    "df.drop(outliers.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Превращаем категорийные признаки в числовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляем столбцы с категорийными признаками, затем заменяем их на числа\n",
    "categorical_columns = df.columns[df.dtypes == 'object']\n",
    "labelencoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = labelencoder.fit_transform(df[column])\n",
    "\n",
    "#Выводим сводную информацию о датафрейме и его столбцах (признаках), чтобы убедиться, что теперь они все содержат цифровые значения\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем целевую переменную, делим датасет на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Назначаем целевой переменной цену 1 кв. метра, а можно и цену всей квартиры, тогда будет y = df['price']\n",
    "y = df['priceMetr']\n",
    "\n",
    "#Создаем список признаков, на основании которых будем строить модели\n",
    "features = [\n",
    "            'wallsMaterial', \n",
    "            'floorNumber', \n",
    "            'floorsTotal', \n",
    "            'totalArea', \n",
    "            'kitchenArea',\n",
    "            'distance',\n",
    "            'azimuth'\n",
    "           ]\n",
    "\n",
    "#Создаем датафрейм, состоящий из признаков, выбранных ранее\n",
    "X = df[features]\n",
    "\n",
    "#Проводим случайное разбиение данных на выборки для обучения (train) и валидации (val), по умолчанию в пропорции 0.75/0.25\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем регрессионную модель случайного леса \n",
    "rf_model = RandomForestRegressor(n_estimators=500, \n",
    "                                 n_jobs=-1,  \n",
    "                                 bootstrap=False,\n",
    "                                 criterion='mse',\n",
    "                                 max_features=6,\n",
    "                                 random_state=1)\n",
    "\n",
    "#Проводим подгонку модели на обучающей выборке \n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "#Вычисляем предсказанные значения цен на основе валидационной выборки\n",
    "rf_prediction = rf_model.predict(val_X)\n",
    "\n",
    "#Вычисляем и печатаем величины ошибок при сравнении известных цен квартир из валидационной выборки с предсказанными моделью\n",
    "print_metrics(rf_prediction, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем регрессионную модель XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:gamma', \n",
    "                             learning_rate = 0.05,\n",
    "                             max_depth = 12, \n",
    "                             n_estimators = 3000,\n",
    "                             nthread = 8,\n",
    "                             eval_metric = 'gamma-nloglik', \n",
    "                             )\n",
    "\n",
    "#Проводим подгонку модели на обучающей выборке \n",
    "xgb_model.fit(train_X, train_y, )\n",
    "\n",
    "#Вычисляем предсказанные значения цен на основе валидационной выборки\n",
    "xgb_prediction = xgb_model.predict(val_X)\n",
    "\n",
    "#Вычисляем и печатаем величины ошибок при сравнении известных цен квартир из валидационной выборки с предсказанными моделью\n",
    "print_metrics(xgb_prediction, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем регрессионную модель LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "                              objective='regression',\n",
    "                              num_leaves=200,\n",
    "                              learning_rate=0.05, \n",
    "                              n_estimators=2000,\n",
    "                              max_bin = 300, \n",
    "                              bagging_fraction = 0.9,\n",
    "                              bagging_freq = 70, \n",
    "                              feature_fraction_seed=5, \n",
    "                              bagging_seed=2,\n",
    "                              min_data_in_leaf = 2, \n",
    "                              boosting_type= 'gbdt',\n",
    "                             )\n",
    "\n",
    "#Проводим подгонку модели на обучающей выборке \n",
    "lgb_model.fit(train_X, train_y)\n",
    "\n",
    "#Вычисляем предсказанные значения цен на основе валидационной выборки\n",
    "lgb_prediction = lgb_model.predict(val_X)\n",
    "\n",
    "#Вычисляем и печатаем величины ошибок при сравнении известных цен квартир из валидационной выборки с предсказанными моделью\n",
    "print_metrics(lgb_prediction, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Усреднение предсказаний моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Усредняем предсказания всех трех моделей\n",
    "prediction = rf_prediction * 0.33 + xgb_prediction * 0.34 + lgb_prediction * 0.33\n",
    "\n",
    "#Вычисляем и печатаем величины ошибок для усредненного предсказания\n",
    "print_metrics(prediction, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Изучаем важность признаков в модели Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Рассчитываем важность признаков в модели Random forest\n",
    "importances = rf_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "#Печатаем рейтинг признаков\n",
    "print(\"Рейтинг важности признаков:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))\n",
    "\n",
    "#Строим столбчатую диаграмму важности признаков\n",
    "plt.figure()\n",
    "plt.title(\"Важность признаков\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], color=\"g\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
